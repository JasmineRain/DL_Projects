R-CNN小结：

1. 预训练直接使用AlexNet的参数，接着使用每张图像生成1K~2K个候选区域，方法为selective search，候选区域与标注框IoU≥0.5为正样本，其余为负样本
    用于fune-tuning（N+1）类输出
2. 完成fine-tuning后，对每个类进行SVM的训练，文章选取了IoU=0.3作为界定正负样本
3. 测试阶段对于每张图片，对每个类独立使用非极大抑制
4. 训练了边框回归


Fast R-CNN小结：

1. 依旧采用selective search方法获取候选框，但是实现了卷积参数共享，通过对应关系从feature map上找出候选框位置
2. 提出RoI池化层（用于替代VGG第5层的普通池化层），去掉SPP的多尺度池化，直接将特征图均分为M*N块，转变为大小固定的特征向量
3. 联合候选框回归和分类的多任务全连接层，分类层输出为N+1类，回归层输出为4N类，表示分别属于N类时的平移缩放
4. 损失函数为分类损失和回归损失的的加权和，分类为背景时不考虑回归损失
5. fine-tuning使用IoU≥0.5的为正样本，其余为负样本，比例为正：负=1：3
6. SVD全连接层加速网络


Faster R-CNN小结：

1. 使用RPN代替了之前的selective search方法得到region proposal
2. 使用ZF net或者VGG net作为基础网络，得到feature map并采用n*n的sliding window处理成256维向量
3. 对sliding window的中心点预测anchor boxes（文中k=9），并将256维特征映射为2k个分类层输出（foreground & background）
    以及4k个回归层输出（RPN输出）
4. anchor box与任意目标的GT的IoU最大 / IoU＞0.7，则被视为正样本；IoU＜0.3为负样本，其余不参与训练
5. 之后得到的候选区域与Fast R-CNN处理一致，输入RoI池化层处理


YOLO v1小结：

1. 不同于生成region proposal的方法，YOLO使用整张图作为输入信息，直接在输出层回归bounding box的位置和类别，把背景误认为目标的错误更少
2. 直接将一幅图分成n*n个网格，如果m某个目标的中心位于这个网格中，则该网格负责预测该目标
3. S*S个网格中 每个网格预测B个bouding box和C个类的信息，其中每个bounding box包括4个位置值和1个confidence值（有目标的置信度和交并比的乘积）
    输出形式为S*S*(5*B+C)(7*7*30)
4. sum squred error loss稍作改进作为损失函数，对小目标、紧靠的目标检测效果不佳，一个网格只有预测了2个框，损失函数有待加强


SSD小结：

1. SSD采用了多尺度特征提取的思想，不断缩小特征图的长宽，越小的特征图感受野越大，以加强对各种大小的物体的检测
2. default box，即anchor boxes，文中采用了不同尺寸和宽高比的default box，并预测其相对于ground truth的偏移，并采取hard negative mining
    对于m*n的特征图，需要输出m*n*k(c+4)维度的特征，其中k为default box数，c为分类数，文中c已经包括背景类，取背景置信度最低的负样本参与训练
    保持比例正：负=1：3
4. smooth L1的location loss + 正负样本的softmax loss


R-FCN小结：

1. 主要思想实现了位置敏感得分图（position-sensitive score map）
2. 对于一个RoI，如果其中存在目标，则将此RoI划分为k*k个区域，R-FCN在g共享卷积层的最后加上了一层卷积层，输出维度为H*W*k²(C+1)
    意思是此RoI对于每个类别，都有k²个得分图，分别对应之前RoI被划分的k*k个区域，之后对k*k个区域的每个区域，
    在其对应的得分图上的相应位置（RoI的左上区域，对第1个得分图的左上区域的所有值平均池化）进行池化操作，得到k²个值，分别对C+1个类进行相同操作
    最后得到k*k*(C+1)个值，对于每个类，取计算好的k*k个得分之和作为总得分，共C+1个总得分，使用softmax函数
3. 同理从共享卷积层最后接上一个并行的得分图，用于进行位置回归，输出维度为H*W*4k²


YOLO v2小结：

1. 每个卷积层后均加入Batch Normalization，并移除了防止过拟合的dropout；去除了v1中的FC层，并使用了anchor boxes提高召回率
2. 在v1中，YOLO先使用224*224分类数据集训练特征提取网络，然后将输入增大到448*448，进而使用检测数据集进行fine-tuning，v2中，在224*224训练过的模型后
    加入了448*448的分类数据集的10个epochs的fine-tuning，再使用448*448的检测数据集进行fine-tuning
3. 使用K-means cluster来选取anchor boxes，在检测训练集上对所有目标框进行k-means聚类最终选取5个anchor boxes
4. 新的encode/decode机制，passthrough层检测细粒度特征，多尺度训练，WordTree


FPN小结：

1. 自下而上的卷积神经网络（下采样），自上而下的过程（上采样），和两部分之间的连接
2. 由于多尺度特征的融合，其对小物体检测精度有大幅度提升


RetinaNet小结：

1. 为了解决样本不平衡的问题，提出了Focal loss，使得数量多的样本对模型有更小的影响，模型更关注数量少的样本
2. 使用Resnet+FPN+2个FCP子网络，输出W*H*








