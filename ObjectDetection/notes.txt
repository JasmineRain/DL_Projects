R-CNN小结：

1. 预训练直接使用AlexNet的参数，接着使用每张图像生成1K~2K个候选区域，方法为selective search，候选区域与标注框IoU≥0.5为正样本，其余为负样本
    用于fune-tuning（N+1）类输出
2. 完成fine-tuning后，对每个类进行SVM的训练，文章选取了IoU=0.3作为界定正负样本
3. 测试阶段对于每张图片，对每个类独立使用非极大抑制
4. 训练了边框回归


Fast R-CNN小结：

1. 依旧采用selective search方法获取候选框，但是实现了卷积参数共享，通过对应关系从feature map上找出候选框位置
2. 提出RoI池化层（用于替代VGG第5层的普通池化层），去掉SPP的多尺度池化，直接将特征图均分为M*N块，转变为大小固定的特征向量
3. 联合候选框回归和分类的多任务全连接层，分类层输出为N+1类，回归层输出为4N类，表示分别属于N类时的平移缩放
4. 损失函数为分类损失和回归损失的的加权和，分类为背景时不考虑回归损失
5. fine-tuning使用IoU≥0.5的为正样本，其余为负样本，比例为正：负=1：3
6. SVD全连接层加速网络


Faster R-CNN小结：

1. 使用RPN代替了之前的selective search方法得到region proposal
2. 使用ZF net或者VGG net作为基础网络，得到feature map并采用n*n的sliding window处理成256维向量
3. 对sliding window的中心点预测anchor boxes（文中k=9），并将256维特征映射为2k个分类层输出（foreground & background）
    以及4k个回归层输出（RPN输出）
4. anchor box与任意目标的GT的IoU最大 / IoU＞0.7，则被视为正样本；IoU＜0.3为负样本，其余不参与训练
5. 之后得到的候选区域与Fast R-CNN处理一致，输入RoI池化层处理


YOLO v1小结：

1. 不同于生成region proposal的方法，YOLO使用整张图作为输入信息，直接在输出层回归bounding box的位置和类别，把背景误认为目标的错误更少
2. 直接将一幅图分成n*n个网格，如果m某个目标的中心位于这个网格中，则该网格负责预测该目标
3. S*S个网格中 每个网格预测B个bouding box和C个类的信息，其中每个bounding box包括4个位置值和1个confidence值（有目标的置信度和交并比的乘积）
    输出形式为S*S*(5*B+C)(7*7*30)
4. sum squred error loss稍作改进作为损失函数，对小目标、紧靠的目标检测效果不佳，一个网格只有预测了2个框，损失函数有待加强


SSD小结：

1. SSD采用了多尺度特征提取的思想，不断缩小特征图的长宽，越小的特征图感受野越大，以加强对各种大小的物体的检测
2. default box，即anchor boxes，文中采用了不同尺寸和宽高比的default box，并预测其相对于ground truth的偏移，并采取hard negative mining
    对于m*n的特征图，需要输出m*n*k(c+4)维度的特征，其中k为default box数，c为分类数，文中c已经包括背景类，取背景置信度最低的负样本参与训练
    保持比例正：负=1：3
4. 损失函数为smooth L1的location loss + 正负样本的softmax loss


R-FCN小结：

1. 主要思想实现了位置敏感得分图（position-sensitive score map）
2. 对于一个RoI，如果其中存在目标，则将此RoI划分为k*k个区域，R-FCN在g共享卷积层的最后加上了一层卷积层，输出维度为H*W*k²(C+1)
    意思是此RoI对于每个类别，都有k²个得分图，分别对应之前RoI被划分的k*k个区域，之后对k*k个区域的每个区域，
    在其对应的得分图上的相应位置（RoI的左上区域，对第1个得分图的左上区域的所有值平均池化）进行池化操作，得到k²个值，分别对C+1个类进行相同操作
    最后得到k*k*(C+1)个值，对于每个类，取计算好的k*k个得分之和作为总得分，共C+1个总得分，使用softmax函数
3. 同理从共享卷积层最后接上一个并行的得分图，用于进行位置回归，输出维度为H*W*4k²


YOLO v2小结：

1. 每个卷积层后均加入Batch Normalization，并移除了防止过拟合的dropout；去除了v1中的FC层，并使用了anchor boxes提高召回率
2. 在v1中，YOLO先使用224*224分类数据集训练特征提取网络，然后将输入增大到448*448，进而使用检测数据集进行fine-tuning，v2中，在224*224训练过的模型后
    加入了448*448的分类数据集的10个epochs的fine-tuning，再使用448*448的检测数据集进行fine-tuning
3. 使用K-means cluster来选取anchor boxes，在检测训练集上对所有目标框进行k-means聚类最终选取5个anchor boxes
4. 新的encode/decode机制，passthrough层检测细粒度特征，多尺度训练，WordTree


FPN小结：

1. 自下而上的卷积神经网络（下采样），自上而下的过程（上采样），和两部分之间的连接
2. 由于多尺度特征的融合，其对小物体检测精度有大幅度提升


RetinaNet小结：

1. 为了解决样本不平衡的问题，提出了Focal loss，使得数量多的样本对模型有更小的影响，模型更关注数量少的样本
2. 使用Resnet+FPN+2个FCP子网络，一个负责分类，输出W*H*KA，；另一个负责边框回归，输出W*H*4A
    其中K为类别数目，A为anchor个数，文中A=9


YOLO v3小结：

1. 只对最佳的anchor box进行边框预测，非最佳且IoU>0.5的不预测边框，同时，每个GT box只匹配一个最佳anchor box
    这个anchor box的分类得分为1（分类使用logistic reg） 未被匹配上的anchor box仅参与分类损失计算
2. 多尺度预测，根据对数据集进行的聚类，得出了9种尺寸的先验框，(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)
    然后根据先验框的尺寸，将他们分配给不同尺度的特征图。v3共有3个不同尺度的feature map，深度都是255，宽高分别是13，26，52
    其中255=3*(5+80)，3为被分配到的先验框个数，80为各类别的概率，5则是每个box的(x，y，w，h，confidence)，对小目标有一定提升
3. 更好的backbone，Darknet-53


Mask R-CNN小结：

1. 保持了two-stage结构，只是另外加入了FCN来产生Mask分支（不同于其他实例分割的先分割后分类）
2. 提出RoIAlign取代RoIPooling，双线性插值法避免了后者中两次引入的量化误差值（图像坐标->feature map坐标；feature map坐标->RoI feature坐标因不整除均产生误差）
3. 损失函数由3个分支组成：其中分类误差和边框回归误差同Faster R-CNN，对于Mask分支，输出为K*m*m（每个RoI），表示K个（K类）m*m的binary mask
    注意Mask损失只计算K个mask中 与GT的类别相同的那个mask，依赖分类分支的预测来选择相应mask（不同于FCN，FCN各个mask之间存在竞争关系）


RefineDet小结：

1. 结合one-stage和two-stage检测方法的优点，提出了ARM、ODM、TCB
2. ARM：识别和删除negative anchors，减少分类器搜索空间，利用多层特征来回归bbox和前景背景二分类，优化初始值
3. ODM：使用ARM的输出产生的refined anchors作为输入，融合不同层的特征，做多类别分类和bbox回归
4. TCB：将ARM的输出feature map转换成ODM的输入
5. 损失函数包括ARM部分的binary cls损失+回归损失，ODM部分的多分类损失和回归损失




M2Det小结：

1.



综述概括点： 年份、会议、名称、论文地址、stage、亮点、输入、输出、特有网络结构、损失函数、anchor box

